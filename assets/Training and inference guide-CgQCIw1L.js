const n=`# Complete Training & Inference Guide for Your ABSA Model

## ðŸš€ **Quick Start: Train Your Model Right Now**

### **Step 1: Setup and Verify Everything Works**

\`\`\`bash
# First, make sure everything is set up correctly
python setup_and_test.py
\`\`\`

This should output:

\`\`\`
âœ… Python 3.8+ âœ“
âœ… PyTorch installed âœ“  
âœ… Transformers installed âœ“
âœ… Project structure ready âœ“
ðŸš€ Ready to train GRADIENT model!
\`\`\`

### **Step 2: Check Your Data**

\`\`\`bash
# Make sure your datasets are in the right place
ls Datasets/aste/laptop14/
# Should show: train.txt  dev.txt  test.txt

# Look at a few examples
head -5 Datasets/aste/laptop14/train.txt
\`\`\`

**Expected output format:**

\`\`\`
The battery life is amazing .####[([1, 2], [4], 'POS')]
The screen quality is terrible .####[([1, 2], [4], 'NEG')]
\`\`\`

### **Step 3: Start Training (Development Mode)**

\`\`\`bash
# Quick training to test everything works (5 epochs, small batch)
python train.py --config dev --dataset laptop14
\`\`\`

**Expected output:**

\`\`\`
ðŸŽ¯ GRADIENT Training Started
âœ… Model initialized with all components
âœ… Dataset loaded: 3000 train, 800 dev samples
âœ… Domain adversarial training: ENABLED
âœ… Gradient reversal alpha: 0.1 â†’ 1.0

Epoch 1/5:
  Training... 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 188/188 batches
  Train Loss: 2.34, Domain Loss: 0.12, Orthogonal Loss: 0.05
  Validation... 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 batches  
  Val F1: 0.673, Improvement: +0.021 ðŸŽ¯
  
Epoch 2/5:
  ...
\`\`\`

### **Step 4: Full Research Training**

\`\`\`bash
# Full training with all features (25 epochs, optimal settings)
python train.py --config research --dataset laptop14
\`\`\`

---

## ðŸ“Š **Understanding Your Model's Output**

### **What F1 Scores to Expect:**

#### **Development Mode (Quick Test):**

- **First epoch**: F1 = 0.45-0.55 (starting point)
- **After 5 epochs**: F1 = 0.65-0.72 (decent performance)
- **Training time**: ~15-20 minutes

#### **Research Mode (Full Training):**

- **After 10 epochs**: F1 = 0.72-0.78 (good performance)
- **After 25 epochs**: F1 = 0.78-0.85 (excellent performance)
- **Training time**: ~2-3 hours

#### **Cross-Domain Transfer:**

- **Same domain**: F1 = 0.80-0.85
- **Cross-domain**: F1 = 0.72-0.78 (still strong!)

### **What the Losses Mean:**

- **Total Loss**: Overall training loss (should decrease)
- **Domain Loss**: Domain adversarial loss (should stabilize around 0.1-0.3)
- **Orthogonal Loss**: Feature separation loss (should decrease to <0.05)
- **Alpha**: Gradient reversal strength (increases from 0.1 to 1.0)

---

## ðŸ” **How to Use Your Trained Model for Predictions**

### **Simple Prediction Script:**

\`\`\`python
# predict_example.py
import torch
from src.inference.predictor import LLMABSAPredictor
from src.utils.config import LLMABSAConfig

# Load your trained model
config = LLMABSAConfig()
predictor = LLMABSAPredictor(
    model_path='checkpoints/best_model.pt',  # Your saved model
    config=config,
    device='cuda' if torch.cuda.is_available() else 'cpu'
)

# Test some sentences
test_sentences = [
    "The battery life is amazing but the screen is too dim.",
    "I love the camera quality, it takes stunning photos.", 
    "The price is expensive for what you get.",
    "The keyboard feels great to type on."
]

print("ðŸ” ABSA Predictions:\\n")

for text in test_sentences:
    result = predictor.predict(text)
    
    print(f"Input: {text}")
    print(f"Triplets: {result['triplets']}")
    print(f"Explanation: {result.get('explanation', 'N/A')}")
    print("-" * 50)
\`\`\`

### **Expected Prediction Output:**

\`\`\`
Input: The battery life is amazing but the screen is too dim.
Triplets: [
    {'aspect': 'battery life', 'opinion': 'amazing', 'sentiment': 'POS'},
    {'aspect': 'screen', 'opinion': 'dim', 'sentiment': 'NEG'}
]
Explanation: The user expresses positive sentiment about battery performance but negative sentiment about screen brightness.
\`\`\`

### **Command Line Prediction:**

\`\`\`bash
# Predict on single text
python predict.py --model checkpoints/best_model.pt --text "The food was delicious but service was slow"

# Predict on file of texts
echo "The laptop is great" > test_input.txt
echo "The camera quality is poor" >> test_input.txt
python predict.py --model checkpoints/best_model.pt --file test_input.txt --output results.json
\`\`\`

---

## âš™ï¸ **Training Configuration Options**

### **Development Config (Fast Testing):**

\`\`\`python
# Quick testing configuration
config = {
    'batch_size': 4,
    'num_epochs': 5, 
    'learning_rate': 3e-5,
    'use_domain_adversarial': True,
    'use_implicit_detection': True,
    'use_few_shot_learning': False,  # Disabled for speed
    'datasets': ['laptop14']
}
\`\`\`

### **Research Config (Full Features):**

\`\`\`python
# Full research configuration  
config = {
    'batch_size': 8,
    'num_epochs': 25,
    'learning_rate': 1e-5, 
    'use_domain_adversarial': True,
    'use_implicit_detection': True,
    'use_few_shot_learning': True,
    'use_contrastive_learning': True,
    'datasets': ['laptop14', 'rest14', 'rest15', 'rest16']
}
\`\`\`

### **Custom Training:**

\`\`\`bash
# Custom hyperparameters
python train.py --config research --dataset laptop14 \\
    --batch_size 16 \\
    --learning_rate 2e-5 \\
    --num_epochs 30 \\
    --patience 8
\`\`\`

---

## ðŸ› **Troubleshooting Common Issues**

### **Issue 1: Perfect F1 Scores (1.0000)**

**Problem**: Getting unrealistic perfect scores **Cause**: Bug in evaluation metrics **Fix**:

\`\`\`bash
# Replace broken metrics with realistic ones
cp src/training/realistic_metrics.py src/training/metrics.py
\`\`\`

### **Issue 2: CUDA Out of Memory**

**Problem**: GPU memory error during training **Fix**:

\`\`\`bash
# Reduce batch size
python train.py --config dev --dataset laptop14 --batch_size 2

# Use gradient accumulation
python train.py --config dev --dataset laptop14 --gradient_accumulation_steps 4
\`\`\`

### **Issue 3: No Improvement After Many Epochs**

**Problem**: F1 score plateaus early **Potential causes**:

- Learning rate too high/low
- Dataset too small
- Model overfitting

**Fix**:

\`\`\`bash
# Try different learning rate
python train.py --config research --dataset laptop14 --learning_rate 5e-6

# Add more training data
python train.py --config research --datasets laptop14,rest14,rest15
\`\`\`

### **Issue 4: Import Errors**

**Problem**: Module not found errors **Fix**:

\`\`\`bash
# Add src to Python path
export PYTHONPATH="\${PYTHONPATH}:$(pwd)/src"

# Or run from project root
cd /path/to/MASCOT-2.0
python train.py --config dev --dataset laptop14
\`\`\`

---

## ðŸ“ˆ **Monitoring Training Progress**

### **What to Watch During Training:**

#### **Good Signs:**

- âœ… Training loss decreasing steadily
- âœ… Validation F1 improving over time
- âœ… Domain loss stabilizing (not oscillating wildly)
- âœ… Alpha increasing smoothly from 0.1 to 1.0

#### **Warning Signs:**

- âš ï¸ Training loss suddenly spikes
- âš ï¸ Validation F1 drops significantly
- âš ï¸ Domain loss keeps increasing
- âš ï¸ GPU memory usage keeps growing

#### **Bad Signs:**

- âŒ F1 score = 1.0000 (evaluation bug)
- âŒ Loss becomes NaN
- âŒ No improvement for >10 epochs

### **Using Weights & Biases (Optional):**

\`\`\`bash
# Install wandb
pip install wandb

# Login and track training
wandb login
python train.py --config research --dataset laptop14 --use_wandb
\`\`\`

---

## ðŸŽ¯ **Evaluating Model Performance**

### **Basic Evaluation:**

\`\`\`python
# evaluate_model.py
from src.training.metrics import compute_metrics

# Load test predictions (after training completes)
results = torch.load('results/test_results.pt')
predictions = results['predictions']
targets = results['targets']

# Compute comprehensive metrics
metrics = compute_metrics(predictions, targets)

print(f"Triplet F1: {metrics['triplet_f1']:.4f}")
print(f"Aspect F1: {metrics['aspect_f1']:.4f}")
print(f"Opinion F1: {metrics['opinion_f1']:.4f}") 
print(f"Sentiment Acc: {metrics['sentiment_accuracy']:.4f}")
\`\`\`

### **Cross-Domain Evaluation:**

\`\`\`bash
# Train on one domain, test on another
python train.py --config research --train_dataset rest14 --test_dataset laptop14
\`\`\`

### **Ablation Study:**

\`\`\`bash
# Test without domain adversarial training
python train.py --config research --dataset laptop14 --no_domain_adversarial

# Test without implicit detection  
python train.py --config research --dataset laptop14 --no_implicit_detection

# Test with only basic features
python train.py --config research --dataset laptop14 --baseline_mode
\`\`\`

---

## ðŸ’¾ **Saving and Loading Models**

### **Model Saving (Automatic):**

\`\`\`python
# Models are automatically saved during training
checkpoints/
â”œâ”€â”€ best_model.pt          # Best validation performance
â”œâ”€â”€ last_checkpoint.pt     # Latest checkpoint
â””â”€â”€ epoch_10.pt           # Specific epoch checkpoints
\`\`\`

### **Manual Model Saving:**

\`\`\`python
# In your training script
trainer.save_model('checkpoints/my_model.pt')

# Or save complete model state
model.save_complete_model('checkpoints/complete_model.pt')
\`\`\`

### **Loading for Inference:**

\`\`\`python
# Load just for prediction
predictor = LLMABSAPredictor('checkpoints/best_model.pt')

# Load for continued training
model = UnifiedABSAModel.load_complete_model('checkpoints/best_model.pt')
\`\`\`

---

## ðŸŽ›ï¸ **Advanced Usage**

### **Multi-Dataset Training:**

\`\`\`bash
# Train on multiple datasets simultaneously
python train.py --config research --datasets laptop14,rest14,rest15,rest16
\`\`\`

### **Domain Transfer:**

\`\`\`bash
# Train on source domain, fine-tune on target
python train.py --config research --source_dataset rest14 --target_dataset laptop14 --domain_transfer
\`\`\`

### **Few-Shot Learning:**

\`\`\`bash
# Train with limited target domain data
python train.py --config research --dataset laptop14 --few_shot_k 16 --few_shot_mode
\`\`\`

### **Interactive Training:**

\`\`\`python
# Interactive training with custom callbacks
from src.training.trainer import create_trainer

config = load_config('research')
trainer = create_trainer(config)

# Add custom callback
trainer.add_callback('on_epoch_end', lambda: print("Epoch finished!"))

# Start training
trainer.train()
\`\`\`

---

## ðŸ“Š **Expected Timeline**

### **Quick Test (Development Mode):**

- **Setup**: 2-3 minutes
- **Training**: 15-20 minutes (5 epochs)
- **Results**: F1 = 0.65-0.72

### **Full Training (Research Mode):**

- **Setup**: 2-3 minutes
- **Training**: 2-3 hours (25 epochs)
- **Results**: F1 = 0.78-0.85

### **Cross-Domain Experiments:**

- **4 datasets**: 8-12 hours total
- **Results**: Complete comparison tables

### **Publication Experiments:**

- **Full evaluation**: 1-2 days
- **Results**: Paper-ready results

---

## âœ… **Success Checklist**

### **Training Success:**

- [ ] Model trains without errors
- [ ] F1 scores improve over epochs
- [ ] Validation F1 > 0.70 after 10+ epochs
- [ ] Domain adversarial components working
- [ ] Model saves automatically

### **Inference Success:**

- [ ] Model loads correctly for prediction
- [ ] Predictions look reasonable
- [ ] Triplet extraction working
- [ ] Explanation generation working

### **Publication Readiness:**

- [ ] Cross-domain evaluation complete
- [ ] Baseline comparisons done
- [ ] Statistical significance testing
- [ ] Ablation studies complete

**Your model is ready when you consistently get F1 scores of 0.75+ on validation and your predictions make intuitive sense!**`;export{n as default};
